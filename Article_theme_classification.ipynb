{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DHuFg4yjZYvz"
      },
      "source": [
        "### **Python Project - Article Theme Classification**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "h2OQzq58Zf6V"
      },
      "source": [
        "**Objective:**\n",
        "\n",
        "To classify the theme of an article."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "M1Go98-o7tnH"
      },
      "source": [
        "- Gather a corpus of articles from a given URL.\n",
        "- Feature Extraction - convert the text to numericals to train the Machine Learning Model.\n",
        "- Train KMeans clustering algorithm.\n",
        "- Figure out different themes from the clusters.\n",
        "- Train K Nearest Neighbors classifier algorithm with the articles data and their themes.\n",
        "- Assign a theme to a new article.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FO4z6Dcj9jFv"
      },
      "source": [
        "## **Gather a corpus of articles from a given URL**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CwTWcKD6UEHv"
      },
      "source": [
        "We need urllib and Beautiful Soup libraries to download and parse the text."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HY1zyAoEZuBR"
      },
      "outputs": [],
      "source": [
        "# to download and parse a HTML webpage\n",
        "import urllib  \n",
        "from bs4 import BeautifulSoup"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "49A0VitMkmgb"
      },
      "source": [
        "### **Get all the article URLs from a blog URL**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4G3_WHlK7WS9"
      },
      "source": [
        "Below is the URL of a blog where few technology related articles are summarized and posted every day.\n",
        " "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AbXLTvSxaFV3"
      },
      "outputs": [],
      "source": [
        "blogUrl = \"http://doxydonkey.blogspot.in\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4ubLfvVm_Rbr"
      },
      "source": [
        "In the blog URL, we have 7 articles and a link at last to retrive the previous articles. Each URL will have the blog posts or tech news summaries for seven days. \n",
        "\n",
        "There is a link on the homepage called Older Posts, which will give a URL with the summaries for the previous seven days. \n",
        "\n",
        "The below function will get all the URL links of the previous articles and append them to the variable `links`."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HR6f9CpWVVXr"
      },
      "source": [
        "These will be represented using a tag. `soup.findAll` will find all the links which are present on the homepage. We want to find the link which has the text called Older Posts. For each link, we can find the URL by using the attribute `href` and the text by using the attribute `title`. If the `title` is equal to `Older Posts`, then we want to actually keep that link. So we append it where we are storing all the links that we will later parse for articles. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "X38HjWWhZxnu"
      },
      "source": [
        "**The below method is customized only to this website and if we want to use it for another website we need to make some changes to the code by inspecting the HTML code of the specific website.**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "C4QAQ87nEyJU"
      },
      "outputs": [],
      "source": [
        "#Gets all the urls from the homepage and append it to a variable\n",
        "def getAllBlogPosts(url,links):\n",
        "    response = urllib.request.urlopen(url)\n",
        "    soup = BeautifulSoup(response)\n",
        "    for a in soup.findAll('a'):\n",
        "        try:\n",
        "            url = a['href']\n",
        "            title = a['title']\n",
        "            if title == \"Older Posts\":\n",
        "                links.append(url)\n",
        "                getAllBlogPosts(url,links)\n",
        "        except:\n",
        "            title = \"\"\n",
        "    return"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ry93cCH1AIRx"
      },
      "source": [
        "We create a list `links` and call the above function by passing the blogURL and the list."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "F6CWsjF4l3a5"
      },
      "outputs": [],
      "source": [
        "#Create a list and store all the article URLs \n",
        "links = []\n",
        "getAllBlogPosts(blogUrl,links)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eJ0gVF3NAbED"
      },
      "source": [
        "The `getBlogText` function gets all the articles from the given URLs. Each URL contains 7 articles and appends all the article texts to the `posts` variable and returns it."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "j2RpARppX61t"
      },
      "source": [
        "To understand how to parse the article text, we can go to the browser and inspect the article text element. Right-click on the article text and click `Inspect`, and this will open up a developer tools window which shows you the HTML corresponding to the element that we are looking at. Each article that we want to collect is a bullet point on this page, and bullet points are represented using the tag `li`. Each day's tech news summary is one post, which might contain multiple such bullet points. Each day's summary is contained within a `div` element whose class name is `post-body`. So in order to collect all the articles, we need to find divs which have this class, post-body, and within those divs, we need to find the bullet points represented by the `li` tag. Given a URL containing multiple tech news summaries, this function will collect all the articles from that blog page."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AUIkN5GkFUpH"
      },
      "outputs": [],
      "source": [
        "#get the article text from the URL\n",
        "def getBlogText(testUrl):\n",
        "    response = urllib.request.urlopen(testUrl)\n",
        "    soup = BeautifulSoup(response)\n",
        "    mydivs = soup.findAll(\"div\", {\"class\":'post-body'})\n",
        "    \n",
        "    posts =[]\n",
        "    for div in mydivs:\n",
        "        posts += map(lambda p:p.text.encode('ascii', errors='replace').replace(b\"?\",b\" \"), div.findAll(\"li\"))\n",
        "    return posts"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "T-J3_1KDBis1"
      },
      "source": [
        "let's create a variable `BlogPosts` to store the corpus of articles.\n",
        "We can iterate over the `links` to extract the articles using the `getBlogText` function and store them in the variable `BlogPosts`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "A-OJQTT2F1oo"
      },
      "outputs": [],
      "source": [
        "BlogPosts = []\n",
        "for link in links:\n",
        "    BlogPosts += getBlogText(link)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "law-yehJCjAX"
      },
      "source": [
        "Let's have a look at first 5 blog posts."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ypub1zUqF3VC",
        "outputId": "dd4fba8e-8eef-4132-a689-9b09d9125fd9"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "[b\"SoftBank's $100 Billion Tech Fund Rankles VCs as Valuations Soar: In the months since Softbank Group Corp. unveiled plans for a $100 billion technology fund, the Japanese company has been making its presence deeply felt across the industry. The Vision Fund closed a few days ago with $93 billion in initial commitments, and already venture firms from London to Silicon Valley are fretting about a behemoth with the resources, clout and name recognition to snatch away the most promising deals. Just last week, SoftBank swooped in and pumped $1.4 billion into Paytm, India s largest digital-payments startup. The deal boosted Paytm's valuation by about 40 percent to $7 billion. That's not outlandish given Paytm's dominant market position, but the valuations of other SoftBank deals have prompted head-scratching and ignited alarm that a funding atmosphere that only recently cooled off will heat up again. there's the concern that SoftBank will ladle out more money than startups need or can absorb. Already founders approached by SoftBank are caught between the desire to take the money and concern about handing over too much control of their company, according to an investor. One startup targeted by SoftBank has tried to negotiate for less money, this person says. SoftBank won't budge; it's a big check or nothing. SoFi originally asked for less money, too, according to another investor. Pushing startups to take more cash than they ask for has been Son's strategy since the beginning. SoftBank invested more money in Yahoo, Alibaba and Didi than what the entrepreneurs had initially wanted. \",\n",
              " b'Quora tests video answers to steal Q&A from YouTube: Newly-minted unicorn Quora has even bigger ambitions than text questions-and-answers. And it s not going to let video giants or startups disrupt its future. This week Quora began testing video answers, because sometimes it s a lot easier to show someone how something works, the best way to complete a task, or why one thing is better than another than try to write it out for them. Users in the beta group will be able to record videos on iOS or Android as supplements or complete answers that everyone on Quora can watch. It s considering allowing video uploads, which might offer more polished content but increase spam concerns. Previously, Quora only let users answer with text, natively hosted photos, links, and embedded videos from platforms like YouTube. Now it s actively hosting and soliciting video uploads. Quora s entry into the space could box out younger competitors like Justin Kan s mobile video Q&A app Whale, and video Ask Me Anything app Yam. These apps are focused entirely on simplifying the process of recording video answers to questions with features like filters to make you look better, and both give creators ways to earn money. But Quora s 190 million users, $226 million in funding, and 8-year head start give it a big edge. It s been cautiously curating a network of experts and content, while building a brand name known for quality in contrast to its predecessor Yahoo Answers. Its network effect may be tough to break.',\n",
              " b'Pittsburgh Welcomed Uber s Driverless Car Experiment. Not Anymore. When Uber picked this former Rust Belt town as the inaugural city for its driverless car experiment, Pittsburgh played the consummate host.  You can either put up red tape or roll out the red carpet,  Bill Peduto, the mayor of Pittsburgh, said in September.  If you want to be a 21st-century laboratory for technology, you put out the carpet.  Nine months later, Pittsburgh residents and officials say Uber has not lived up to its end of the bargain. Among Uber s perceived transgressions: The company began charging for driverless rides that were initially pitched as free. It also withdrew support from Pittsburgh s application for a $50 million federal grant to revamp transportation. And it has not created the jobs it proposed in a struggling neighborhood that houses its autonomous car testing track. The deteriorating relationship between Pittsburgh and Uber offers a cautionary tale, especially as other cities consider rolling out driverless car trials from Uber, Alphabet s Waymo and others.',\n",
              " b'LeEco employees are being called to a Tuesday meeting, and massive layoffs are expected: LeEco, a Chinese company that made a big splash in the U.S. last fall, is preparing for a round of layoffs that may happen as soon as Tuesday, according to sources. Two people told CNBC the company is planning massive layoffs in the U.S., with one source saying that only 60 employees will be left after the cut. The company\\'s current headcount in the U.S. is over 500, according to this person. LeEco started out in China as a streaming media provider   it has been referred to as the \"Netflix of China\"   and looked to expand into the US by selling affordable hardware that linked consumers to media content from LeEco\\'s partners. Its first batch of products included two smartphones and several TVs, all of which offered flagship-level specs at affordable prices. The idea, it seemed, was that LeEco would make its money back when consumers tuned in to partner programming. When it made its debut in the US in October 2016, it also promised more, including VR headsets and an electric bicycle.',\n",
              " b'Why Did a Chinese Peroxide Company Pay $1 Billion for a Talking Cat  Even by the opaque standards of Chinese mergers and acquisitions, the deal was a head-scratcher. It s hard to see the synergies between a maker of chemical solvents and a digital cat perched over a toilet. And curiously, the buyer, which had recently been renamed Zhejiang Jinke Entertainment Culture Co., had revenue of only $133 million in 2016, according to Bloomberg data pulled from regulatory filings, and its gross profit was $55 million. Jinke won t say where the money to buy Outfit7 came from. Talking Tom is not alone. There s been a recent flurry of oddball pairings between Chinese industrial interests and Western entertainment companies. A real estate magnate in Beijing bought Legendary Entertainment, the movie studio that made the Dark Knight trilogy, for $3.5 billion. A maker of construction materials bought Framestore, the company behind the special effects in the Harry Potter films. Zhejiang Dragon Pipe Manufacturing Co. acquired app developer Entertainment Game Labs. And perhaps strangest of all, Digital Extremes Ltd., which created an alien battle game, and the studio Splash Damage Ltd., which made an offshoot of the Xbox hit Gears of War, were bought by an enormous Chinese poultry processor. According to CODE Advisors LLC, an investment bank that specializes in media and technology deals, 70 percent of all acquisitions of game companies since 2015 have been by Chinese buyers. Samo, for one, isn t stopping to ask questions. A vegan, he s using his windfall to start a food-sustainability foundation.  It s not easy,  he says,  to find buyers for a $1 billion company.  The deal activity can best be understood as a consequence of quirks in the Chinese stock market. In China, industrial companies trade at valuations they d never receive elsewhere in the world. Affan Butt, an investment banker who helped facilitate the sale of Jagex, says some may trade at as much as 100 times their annual earnings more than four times the multiple of General Electric Co. This means they can acquire companies at what is effectively a discount. A target like Jagex is worth more once it s part of a Chinese-listed company, allowing the acquirer to pay prices that appear bafflingly high to the rest of the world. ']"
            ]
          },
          "execution_count": 7,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "BlogPosts[0:5]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KpNjhlQvCr-3"
      },
      "source": [
        "As the above string is in binary format, we can now convert all the strings to normal format."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "q1P0WPxuT7kW",
        "outputId": "dd57a628-71d3-4e9d-bc6c-8d8f1aff3d6c"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "[\"SoftBank's $100 Billion Tech Fund Rankles VCs as Valuations Soar: In the months since Softbank Group Corp. unveiled plans for a $100 billion technology fund, the Japanese company has been making its presence deeply felt across the industry. The Vision Fund closed a few days ago with $93 billion in initial commitments, and already venture firms from London to Silicon Valley are fretting about a behemoth with the resources, clout and name recognition to snatch away the most promising deals. Just last week, SoftBank swooped in and pumped $1.4 billion into Paytm, India s largest digital-payments startup. The deal boosted Paytm's valuation by about 40 percent to $7 billion. That's not outlandish given Paytm's dominant market position, but the valuations of other SoftBank deals have prompted head-scratching and ignited alarm that a funding atmosphere that only recently cooled off will heat up again. there's the concern that SoftBank will ladle out more money than startups need or can absorb. Already founders approached by SoftBank are caught between the desire to take the money and concern about handing over too much control of their company, according to an investor. One startup targeted by SoftBank has tried to negotiate for less money, this person says. SoftBank won't budge; it's a big check or nothing. SoFi originally asked for less money, too, according to another investor. Pushing startups to take more cash than they ask for has been Son's strategy since the beginning. SoftBank invested more money in Yahoo, Alibaba and Didi than what the entrepreneurs had initially wanted. \",\n",
              " 'Quora tests video answers to steal Q&A from YouTube: Newly-minted unicorn Quora has even bigger ambitions than text questions-and-answers. And it s not going to let video giants or startups disrupt its future. This week Quora began testing video answers, because sometimes it s a lot easier to show someone how something works, the best way to complete a task, or why one thing is better than another than try to write it out for them. Users in the beta group will be able to record videos on iOS or Android as supplements or complete answers that everyone on Quora can watch. It s considering allowing video uploads, which might offer more polished content but increase spam concerns. Previously, Quora only let users answer with text, natively hosted photos, links, and embedded videos from platforms like YouTube. Now it s actively hosting and soliciting video uploads. Quora s entry into the space could box out younger competitors like Justin Kan s mobile video Q&A app Whale, and video Ask Me Anything app Yam. These apps are focused entirely on simplifying the process of recording video answers to questions with features like filters to make you look better, and both give creators ways to earn money. But Quora s 190 million users, $226 million in funding, and 8-year head start give it a big edge. It s been cautiously curating a network of experts and content, while building a brand name known for quality in contrast to its predecessor Yahoo Answers. Its network effect may be tough to break.',\n",
              " 'Pittsburgh Welcomed Uber s Driverless Car Experiment. Not Anymore. When Uber picked this former Rust Belt town as the inaugural city for its driverless car experiment, Pittsburgh played the consummate host.  You can either put up red tape or roll out the red carpet,  Bill Peduto, the mayor of Pittsburgh, said in September.  If you want to be a 21st-century laboratory for technology, you put out the carpet.  Nine months later, Pittsburgh residents and officials say Uber has not lived up to its end of the bargain. Among Uber s perceived transgressions: The company began charging for driverless rides that were initially pitched as free. It also withdrew support from Pittsburgh s application for a $50 million federal grant to revamp transportation. And it has not created the jobs it proposed in a struggling neighborhood that houses its autonomous car testing track. The deteriorating relationship between Pittsburgh and Uber offers a cautionary tale, especially as other cities consider rolling out driverless car trials from Uber, Alphabet s Waymo and others.',\n",
              " 'LeEco employees are being called to a Tuesday meeting, and massive layoffs are expected: LeEco, a Chinese company that made a big splash in the U.S. last fall, is preparing for a round of layoffs that may happen as soon as Tuesday, according to sources. Two people told CNBC the company is planning massive layoffs in the U.S., with one source saying that only 60 employees will be left after the cut. The company\\'s current headcount in the U.S. is over 500, according to this person. LeEco started out in China as a streaming media provider   it has been referred to as the \"Netflix of China\"   and looked to expand into the US by selling affordable hardware that linked consumers to media content from LeEco\\'s partners. Its first batch of products included two smartphones and several TVs, all of which offered flagship-level specs at affordable prices. The idea, it seemed, was that LeEco would make its money back when consumers tuned in to partner programming. When it made its debut in the US in October 2016, it also promised more, including VR headsets and an electric bicycle.',\n",
              " 'Why Did a Chinese Peroxide Company Pay $1 Billion for a Talking Cat  Even by the opaque standards of Chinese mergers and acquisitions, the deal was a head-scratcher. It s hard to see the synergies between a maker of chemical solvents and a digital cat perched over a toilet. And curiously, the buyer, which had recently been renamed Zhejiang Jinke Entertainment Culture Co., had revenue of only $133 million in 2016, according to Bloomberg data pulled from regulatory filings, and its gross profit was $55 million. Jinke won t say where the money to buy Outfit7 came from. Talking Tom is not alone. There s been a recent flurry of oddball pairings between Chinese industrial interests and Western entertainment companies. A real estate magnate in Beijing bought Legendary Entertainment, the movie studio that made the Dark Knight trilogy, for $3.5 billion. A maker of construction materials bought Framestore, the company behind the special effects in the Harry Potter films. Zhejiang Dragon Pipe Manufacturing Co. acquired app developer Entertainment Game Labs. And perhaps strangest of all, Digital Extremes Ltd., which created an alien battle game, and the studio Splash Damage Ltd., which made an offshoot of the Xbox hit Gears of War, were bought by an enormous Chinese poultry processor. According to CODE Advisors LLC, an investment bank that specializes in media and technology deals, 70 percent of all acquisitions of game companies since 2015 have been by Chinese buyers. Samo, for one, isn t stopping to ask questions. A vegan, he s using his windfall to start a food-sustainability foundation.  It s not easy,  he says,  to find buyers for a $1 billion company.  The deal activity can best be understood as a consequence of quirks in the Chinese stock market. In China, industrial companies trade at valuations they d never receive elsewhere in the world. Affan Butt, an investment banker who helped facilitate the sale of Jagex, says some may trade at as much as 100 times their annual earnings more than four times the multiple of General Electric Co. This means they can acquire companies at what is effectively a discount. A target like Jagex is worth more once it s part of a Chinese-listed company, allowing the acquirer to pay prices that appear bafflingly high to the rest of the world. ']"
            ]
          },
          "execution_count": 8,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "#convert the binary strings to normal strings\n",
        "for i,string in enumerate(BlogPosts):\n",
        "  BlogPosts[i] = string.decode('ascii')\n",
        "\n",
        "BlogPosts[0:5]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CWIEcfUQLA98"
      },
      "source": [
        "## **Feature extraction**\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XtoYDq8sEAF8"
      },
      "source": [
        "The process of extracting numeric attributes from text is called feature extraction.\n",
        "\n",
        "There are two methods, **Term Frequency** and **TF-IDF**.\n",
        "\n",
        "Both the methods require a bag of words model means to create a list representing the universe of all words that can appear in any text from the corpus."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Q3DAWRmZInCL"
      },
      "source": [
        "<img src='https://www.romainberg.com/wp-content/uploads/TF_IDF-final-1024x399.png'>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IFmnCPSIjBfZ"
      },
      "source": [
        "We first need to take all the articles and represent them using the TF-IDF representation. \n",
        "\n",
        "`Scikit-learn` is a Python module with a lot of built-in functionality available for machine learning tasks, and this contains a feature extraction module, which allows you to perform TF-IDF representation. \n",
        "\n",
        "So we import the `TfidfVectorizer` from `sklearn.feature_extraction.text`, and we instantiate a vectorizer object."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Md-am4BOGDSS"
      },
      "outputs": [],
      "source": [
        "from sklearn.feature_extraction.text import TfidfVectorizer"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SNT-caMXKQZu"
      },
      "source": [
        "We need to mention the stop words parameter to ignore the stop words from the articles corpus."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "v6t8hqN_LFCA"
      },
      "outputs": [],
      "source": [
        "vectorizer = TfidfVectorizer(max_df=0.5,min_df=2,stop_words='english')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "r6gH4kW6KHKV"
      },
      "source": [
        "This vectorizer has a method called fit_transform, which takes a list of strings and then returns a two-dimensional array in which each row represents one article."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "w7P1WOEYLFm3",
        "outputId": "0ecba5b9-d141-4f88-9246-2b72c42a7f2b"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "(2804, 13220)"
            ]
          },
          "execution_count": 11,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "X = vectorizer.fit_transform(BlogPosts)\n",
        "X.shape"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "auUvB_2fK72S"
      },
      "source": [
        "The shape of the X is (2804, 13220). \n",
        "\n",
        "Here, \n",
        "- 2804 represents the number of articles in the corpus. \n",
        "- 13220 represents the number of distinct words which are present in all articles. "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PSO3tS1NLG9S",
        "outputId": "5bc57f7d-f04c-4929-b057-478881023651"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "(1, 13220)"
            ]
          },
          "execution_count": 12,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "X[0].shape"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zgePqmwvkHXt"
      },
      "source": [
        "the decimal numbers below are the TF-IDF values which represent the particular text."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 139
        },
        "id": "mUkNxRUFcQWk",
        "outputId": "d6efc046-492d-4d3e-b4e4-bbc0bde7a7f1"
      },
      "outputs": [
        {
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "\"SoftBank's $100 Billion Tech Fund Rankles VCs as Valuations Soar: In the months since Softbank Group Corp. unveiled plans for a $100 billion technology fund, the Japanese company has been making its presence deeply felt across the industry. The Vision Fund closed a few days ago with $93 billion in initial commitments, and already venture firms from London to Silicon Valley are fretting about a behemoth with the resources, clout and name recognition to snatch away the most promising deals. Just last week, SoftBank swooped in and pumped $1.4 billion into Paytm, India s largest digital-payments startup. The deal boosted Paytm's valuation by about 40 percent to $7 billion. That's not outlandish given Paytm's dominant market position, but the valuations of other SoftBank deals have prompted head-scratching and ignited alarm that a funding atmosphere that only recently cooled off will heat up again. there's the concern that SoftBank will ladle out more money than startups need or can absorb. Already founders approached by SoftBank are caught between the desire to take the money and concern about handing over too much control of their company, according to an investor. One startup targeted by SoftBank has tried to negotiate for less money, this person says. SoftBank won't budge; it's a big check or nothing. SoFi originally asked for less money, too, according to another investor. Pushing startups to take more cash than they ask for has been Son's strategy since the beginning. SoftBank invested more money in Yahoo, Alibaba and Didi than what the entrepreneurs had initially wanted. \""
            ]
          },
          "execution_count": 13,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "BlogPosts[0]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7i1obRMXctxN",
        "outputId": "7b1c42ca-d2eb-4b1d-81d9-c242a0e96861"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "(2804, 13220)"
            ]
          },
          "execution_count": 14,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "X.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ouNL4taFLKUg",
        "outputId": "2762a1f0-57cf-4819-c045-1beda04db5f0"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "  (0, 1965)\t0.0529628954253893\n",
            "  (0, 12067)\t0.055052384601166085\n",
            "  (0, 4187)\t0.0584507917180868\n",
            "  (0, 8972)\t0.07600199422007031\n",
            "  (0, 3024)\t0.05736900975819227\n",
            "  (0, 9356)\t0.05279111734066146\n",
            "  (0, 6749)\t0.03958895210959883\n",
            "  (0, 1950)\t0.043600047428625034\n",
            "  (0, 2069)\t0.043452319046352085\n",
            "  (0, 4601)\t0.054246249916468064\n",
            "  (0, 7932)\t0.07555595918260936\n",
            "  (0, 3301)\t0.08686905713475819\n",
            "  (0, 2322)\t0.0836816938182973\n",
            "  (0, 4168)\t0.05367794768088572\n",
            "  (0, 11192)\t0.03238529318176009\n",
            "  (0, 13146)\t0.019059438993661963\n",
            "  (0, 210)\t0.08686905713475819\n",
            "  (0, 7579)\t0.04294208101613297\n",
            "  (0, 128)\t0.07062282759807993\n",
            "  (0, 4124)\t0.06173325831827569\n",
            "  (0, 12886)\t0.04413318293366655\n",
            "  (0, 3210)\t0.06382995834007718\n",
            "  (0, 7132)\t0.041567635996890104\n",
            "  (0, 7254)\t0.028166537825240253\n",
            "  (0, 4877)\t0.06584998356087884\n",
            "  :\t:\n",
            "  (0, 5348)\t0.05091405493600229\n",
            "  (0, 6953)\t0.08858068218774492\n",
            "  (0, 5417)\t0.03738918655828943\n",
            "  (0, 9373)\t0.0976446354406848\n",
            "  (0, 11873)\t0.10331584492016813\n",
            "  (0, 1019)\t0.056865310997346596\n",
            "  (0, 1733)\t0.047268651707825834\n",
            "  (0, 12436)\t0.05903405609574837\n",
            "  (0, 7611)\t0.08120938275733285\n",
            "  (0, 7947)\t0.05964974029032647\n",
            "  (0, 13176)\t0.09516745760679785\n",
            "  (0, 11225)\t0.07352968315910585\n",
            "  (0, 1121)\t0.40993376773105983\n",
            "  (0, 12693)\t0.29225427011189004\n",
            "  (0, 11869)\t0.05789670303740555\n",
            "  (0, 9395)\t0.5857718567280811\n",
            "  (0, 13134)\t0.05035294949477881\n",
            "  (0, 1291)\t0.054641987145590216\n",
            "  (0, 1730)\t0.03252123923736035\n",
            "  (0, 11199)\t0.04374977220074543\n",
            "  (0, 7694)\t0.0315777363207523\n",
            "  (0, 5223)\t0.0380000203797898\n",
            "  (0, 5694)\t0.0447756506977566\n",
            "  (0, 12920)\t0.033522646240102906\n",
            "  (0, 5524)\t0.03364307638805961\n"
          ]
        }
      ],
      "source": [
        "print (X[1])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jt42kmXNDVOh"
      },
      "source": [
        "## **Train KMeans clustering algorithm**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "S2vz9bspDTjq"
      },
      "source": [
        " **Clustering:**\n",
        "\n",
        "Here we are given a large group of articles, and we need to divide these articles into clusters or groups on the basis of some common attributes. We want all articles which represent some particular theme to be put into one group. This is a classic example of a clustering problem. \n",
        "\n",
        "Whenever you encounter a large number of items and the objective is to divide them into groups based on some measure of similarity, you're basically solving a clustering problem. \n",
        "\n",
        "The end objective of clustering is to create groups such that items within one group are similar to one another and items which are in different groups are dissimilar to one another. \n",
        "\n",
        "In other words, if you have some metric called similarity, which measures how similar items are to one another, you want to maximize the intracluster similarity, maximize the similarity of items within a cluster, and you want to minimize intercluster similarity. You want to minimize the similarity between items which are in different clusters."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bwfawOfjlLhK"
      },
      "source": [
        "Scikit-learn has a built-in module for clustering called the `sklearn.cluster` module. \n",
        "\n",
        "Within this module, we have a class for the K-Means clustering algorithm. So we can import that class and instantiate a K-Means clustering object. This sets up the algorithm with all the parameters - \n",
        "\n",
        "`n_clusters` is the parameter that is used to specify the number of clusters, which here is three, because we want to divide our articles into three groups. \n",
        "\n",
        "The `init` parameter specifies an algorithm to help choose the initial centroids or means in such a way that we can find the relevant clusters with a minimum number of iterations. \n",
        "\n",
        "We're also specifying the maximum number of iterations, so in case the algorithm does not reach convergence until the 100th iteration, then it will stop at that point.\n",
        "\n",
        "`n_init` - Number of time the k-means algorithm will be run with different centroid seeds. Default value is 10."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EOc-C8eoLMyS"
      },
      "outputs": [],
      "source": [
        "from sklearn.cluster import KMeans\n",
        "km = KMeans(n_clusters = 3, init = 'k-means++', max_iter = 100, n_init = 1, verbose = True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yDUvSzr-mlmf"
      },
      "source": [
        "To perform K-Means clustering, we take our documents represented in TF-IDF, which is the array X, and pass it to the fit method of the K-Means clustering algorithm. "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "26fq5K6kLSj-",
        "outputId": "fb61db81-93b9-4ef1-ef80-063b422b8f9f"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Initialization complete\n",
            "Iteration 0, inertia 5266.672075908238\n",
            "Iteration 1, inertia 2685.7228855572725\n",
            "Iteration 2, inertia 2677.5563626124367\n",
            "Iteration 3, inertia 2674.145183603303\n",
            "Iteration 4, inertia 2671.0745860108595\n",
            "Iteration 5, inertia 2669.3755496569056\n",
            "Iteration 6, inertia 2668.6988850993375\n",
            "Iteration 7, inertia 2668.4275202310287\n",
            "Iteration 8, inertia 2668.266815176653\n",
            "Iteration 9, inertia 2668.179201175489\n",
            "Iteration 10, inertia 2668.098142680158\n",
            "Iteration 11, inertia 2668.0405470171218\n",
            "Iteration 12, inertia 2667.9643083983756\n",
            "Iteration 13, inertia 2667.874353005027\n",
            "Iteration 14, inertia 2667.8199574164273\n",
            "Iteration 15, inertia 2667.7976527097017\n",
            "Iteration 16, inertia 2667.7792048889432\n",
            "Iteration 17, inertia 2667.770560499607\n",
            "Iteration 18, inertia 2667.7685426847183\n",
            "Converged at iteration 18: strict convergence.\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "KMeans(max_iter=100, n_clusters=3, n_init=1, verbose=True)"
            ]
          },
          "execution_count": 17,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "km.fit(X)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "q7yDiLoJm3eX"
      },
      "source": [
        "Every document in our array X has now been assigned a number, which represents the cluster to which it belongs. These numbers are stored in the array labels, which is an attribute of the K-Means object.\n",
        "\n",
        "We also have the counts which represent how many articles are present in each cluster."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0HiqhKr6LTza",
        "outputId": "f64ca7cf-6b7c-4e97-e14a-8916090fd8fa"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "(array([0, 1, 2], dtype=int32), array([ 674, 1681,  449]))"
            ]
          },
          "execution_count": 18,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "import numpy as np\n",
        "np.unique(km.labels_, return_counts=True) "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-xyOMQTNLc_Z"
      },
      "source": [
        "## **Find out different themes from the clusters**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ab5aCHyOupwl"
      },
      "source": [
        "We have to actually look at the articles which are present in each cluster to identify what meaningful theme is represented by each cluster. \n",
        "\n",
        "So let's find some of the important keywords which occur in each cluster to see if we can articulate what those underlying themes might be. \n",
        "\n",
        "We'll set up a dictionary called `text` in which the keys will be the cluster numbers and the values will be the aggregated text across all the articles which are present in that cluster. We'll go through the array of labels, which have the cluster numbers assigned to each document and then collect the text for each document into the corresponding cluster. \n",
        "\n",
        "The `enumerate` function converts the array of labels into a list of tuples where the first element is the index of an article. So using the index, we can get the corresponding article from the list `BlogPosts`. Then we aggregate this text for every article into the corresponding value in the text dictionary. "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sXAG_ufyxVqv",
        "outputId": "08bdcb3d-56ae-4b36-f919-ba0327383313"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "array([0, 1, 0, ..., 1, 1, 1], dtype=int32)"
            ]
          },
          "execution_count": 19,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "km.labels_"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YAM5bqIbLZBU"
      },
      "outputs": [],
      "source": [
        "text={}\n",
        "for i,cluster in enumerate(km.labels_):\n",
        "    oneDocument = BlogPosts[i]\n",
        "    if cluster not in text.keys():\n",
        "        text[cluster] = oneDocument\n",
        "    else:\n",
        "        text[cluster] += oneDocument"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "8Zkus7uILbTu"
      },
      "outputs": [],
      "source": [
        "text"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "spva9ui-yitz"
      },
      "source": [
        "We can use some NLTK functions to find out the most frequent words within each cluster."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "L3O8YEavLrkn"
      },
      "outputs": [],
      "source": [
        "from nltk.tokenize import sent_tokenize, word_tokenize\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.probability import FreqDist\n",
        "\n",
        "from string import punctuation\n",
        "from heapq import nlargest\n",
        "import nltk "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "GclmVIHGLywJ",
        "outputId": "d09d05e3-57e5-44aa-fa59-799633cd66d5"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "execution_count": null,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "nltk.download('stopwords')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vjyda_MnyrjK"
      },
      "source": [
        "When we find the most frequent words, we don't want to include stop words, so we'll set up a variable to represent all the stop words that we want to ignore.\n",
        "\n",
        "Along with the standard list of stop words from English and punctuation, we have some additional words which are common to tech news articles."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "RBsMW0E1Lxb8"
      },
      "outputs": [],
      "source": [
        "_stopwords = set(stopwords.words('english') + list(punctuation)+[\"million\",\"billion\",\"year\",\"millions\",\"billions\",\"y/y\",\"'s\",\"''\",\"``\"])\n",
        " "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "hxPeRR2AMdYl",
        "outputId": "f8e2ed0c-c973-4860-b8c8-c58b058c2285"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "execution_count": null,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        " nltk.download('punkt')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SojrxzNQK3fp"
      },
      "source": [
        "This bit of code here will take the text from each cluster and find out the top 100 words that occur within that text. \n",
        "\n",
        "We iterate through each cluster, and for each cluster, we take the corresponding text and tokenize it into words. We filter out all the stop words and keep only relevant words. \n",
        "\n",
        "Then we use the `FreqDist` function to compute the frequency distribution of the words. \n",
        "\n",
        "We take the `nlargest` function and pick the top 100 words from this frequency distribution. Along with the top 100 keywords, we also separately store the complete frequency distribution, which will keep the words along with their counts in the dictionary. "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Cv2RWwCgL3FP"
      },
      "outputs": [],
      "source": [
        "#to find the top 100 words that occurs frequently in a cluster excluding the stop words\n",
        "keywords = {}\n",
        "counts={}\n",
        "for cluster in range(3):\n",
        "    word_sent = word_tokenize(text[cluster].lower())\n",
        "    word_sent=[word for word in word_sent if word not in _stopwords]\n",
        "    freq = FreqDist(word_sent)\n",
        "    keywords[cluster] = nlargest(100, freq, key=freq.get)\n",
        "    counts[cluster]=freq"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "N6KZMbiDMKFd"
      },
      "source": [
        "Now that we have the 100 most important keywords from each cluster, let's find the 10 keywords which are unique to each cluster.\n",
        "\n",
        "As we iterate through each cluster, we find the list of other clusters. So if we look at cluster zero, we collect all the keywords which are present in the other clusters and we remove those keys from the list of keywords in our cluster. From the remaining keywords, we pick the top 10 most frequently occurring keywords. "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ByqYWCxjMZJH"
      },
      "outputs": [],
      "source": [
        "#Let's find out the top 10 frequently occurred unique keywords from each cluster\n",
        "unique_keys={}\n",
        "for cluster in range(3):   \n",
        "    other_clusters=list(set(range(3))-set([cluster]))\n",
        "    keys_other_clusters=set(keywords[other_clusters[0]]).union(set(keywords[other_clusters[1]]))\n",
        "    unique=set(keywords[cluster])-keys_other_clusters\n",
        "    unique_keys[cluster]=nlargest(10, unique, key=counts[cluster].get)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "u-6KOlkIVB4A"
      },
      "outputs": [],
      "source": [
        "unique_keys"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NRl8aQRyRMYf"
      },
      "source": [
        "**One of the clusters** seems to be related to social media and advertising-related keywords. So articles that covered those themes would reside in this cluster.\n",
        "\n",
        "\n",
        "**Next cluster** seems to have words related to price and profit, which are stock performance-related. So articles about the stock performance might be a part of this cluster.  \n",
        "\n",
        "**The other cluster** has words like round, capital, funding, and valuation. So this is a cluster that deals with startups and the budget and investments they are getting. So any news articles related to these topics would reside within this cluster.\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Aklqtd6KON2b"
      },
      "source": [
        "## **Train K Nearest Neighbors classifier**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pMiuWl4sQf3G"
      },
      "source": [
        "Now that we have different themes that we have identified from our body of articles, we can take any new article and then assign one of these themes to that article. \n",
        "\n",
        "Our classification problem statement here is that given an article, we want to pass it to a classifier, and the output should be one of the themes, theme one, two, or three that we have identified from our clustering step.\n",
        "\n",
        "We take all of our historical data, which is our body of articles, and represent it using the TF-IDF representation. These articles have labels already assigned to them by the clustering step. This training data along with the labels is fed to a standard algorithm in the training step, and that creates the model that can be used in the test step."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HWFjEjqFSY3L"
      },
      "source": [
        "We can import the `KNeighborsClassifier` from scikit-learns' neighbors module. This is a built-in class that helps to perform the classification.\n",
        "\n",
        "So we can instantiate a `KNeighborsClassifier` object and use the fit method to set up the training phase. In the training phase, you need to parse in the complete set of articles for which the labels are already known. \n",
        "\n",
        "So variable `X` has our articles represented as TF-IDF tuples, and `km.labels` is an array with all the cluster numbers assigned to those articles. "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RgCywLJWXgWA"
      },
      "outputs": [],
      "source": [
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "classifier = KNeighborsClassifier(n_neighbors=3)\n",
        "classifier.fit(X,km.labels_)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5AaaRSO3Ta_D"
      },
      "source": [
        "Let's take an article from any website and assign it to a variable article."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cJDH3ufmVDiP"
      },
      "outputs": [],
      "source": [
        "article = \"With Snapchat, Twitter, Instagram, and even Facebook, teens are in constant communication. While many will argue that it is important to have this connection outside of face-to-face interaction, studies have said that eliminating in-person communication can impair critical social skills. But what does this really mean? When teens receive messages via social media, all they see is a screen. They can’t see the person on the other side. They are oblivious to the other person’s reaction. Body language, facial expression, even tone of voice, are removed from the conversation. This is why teens choose to send that risky message or that mean reply, things they wouldn’t ever say to someone’s face. Of course, this creates a problem. Not only are the simplest parts of communication invisible, but it has become easier and easier to hide behind a mask on social media. Social media allows anyone to be anonymous, identity erased. Cyberbullies can create fakes accounts, which allows the bully to say anything without facing consequences. Statistics say that one half of teens have been victims of bullying online and one third have sent rude or harassing comments via social media, (Teens, Social Media & Technology Overview 2015). When teens don’t know the person harassing them it is harder to ask for help or tell an adult.\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "puMFB1zxTRJR"
      },
      "source": [
        "Now before we parse our test article to the `KNeighborsClassifier`, we need to represent that article using the TF-IDF representation. So we use `vectorizer.transform` to convert the article into the TF-IDF form. \n",
        "\n",
        "The test variable will be an array which has one row and as many columns as there are the total number of words in our corpus. "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ChX9jjLyXiZE"
      },
      "outputs": [],
      "source": [
        "test=vectorizer.transform([article.encode('ascii',errors='ignore')])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QvL9Cb5ETyu4"
      },
      "source": [
        "Let's check the shape of the test variable."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6UcTRKMtXsq2",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "305750e8-c82c-423a-d016-250a1b1c8bfc"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(1, 13220)"
            ]
          },
          "metadata": {},
          "execution_count": 39
        }
      ],
      "source": [
        "test.shape"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eXYe7M3uTu6f"
      },
      "source": [
        "Now we can use the predict method of the classifier and that will assign a theme to our article. "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "beOEhRK3YAKU",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "cf4d7202-11e9-4b1a-bca4-ec6c2eb5c8cd"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([1], dtype=int32)"
            ]
          },
          "metadata": {},
          "execution_count": 40
        }
      ],
      "source": [
        "classifier.predict(test)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nOPrGRGfT5pq"
      },
      "source": [
        "Here we can see that the article is assigned to a cluster that describes about the social media and advertizing cluster."
      ]
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [],
      "name": "Article theme classification.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}